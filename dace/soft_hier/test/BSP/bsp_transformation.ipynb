{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://spcl.github.io/dace-webclient/dist/sdfv.js\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dace\n",
    "import typing\n",
    "import os\n",
    "import numpy as np\n",
    "import ast\n",
    "from dace.transformation.dataflow import DoubleBuffering, MapTiling\n",
    "from dace.transformation.soft_hier import SystolocTransformer, SystolicTransformer, SystolicSplitStore, SummaTransformer, BSPTransformer, TESTBSPTransformer\n",
    "from dace.soft_hier import generate_arg_cfg, make_preload_elf, make_preload_elf_hbm_interleaved_new, InterleaveHandler\n",
    "from dace.soft_hier import generate_systolic_BSP, generate_cannon_BSP, generate_summa_BSP\n",
    "from dace.properties import CodeBlock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _my_gen_matmul_sdfg(hardware_matmul_mnk: typing.Tuple,\n",
    "                     global_storage: dace.dtypes.StorageType,\n",
    "                     local_storage: dace.dtypes.StorageType,\n",
    "                     device_schedule: dace.dtypes.ScheduleType,\n",
    "                     thread_group_schedule: dace.dtypes.ScheduleType,\n",
    "                     thread_group_dims: typing.Tuple,\n",
    "                     hbm_split_scheme: typing.List[typing.Tuple[int, int]],\n",
    "                     hbm_placement_scheme: typing.List[typing.Tuple[int, int]],\n",
    "                     input_float,\n",
    "                     output_float,\n",
    "                     coarsening_factor,\n",
    "                     mmad_tasklet_str: str,\n",
    "                     is_hbm_interleaved: bool = False):\n",
    "    sdfg = dace.SDFG(\"GEMM\")\n",
    "    tM, tN, tK = hardware_matmul_mnk\n",
    "    tM *= coarsening_factor\n",
    "    tN *= coarsening_factor\n",
    "    tK *= coarsening_factor\n",
    "    gM, gN = thread_group_dims\n",
    "\n",
    "    main_state = sdfg.add_state(\"main\")\n",
    "    state = main_state\n",
    "\n",
    "    arrs = dict()\n",
    "    for arr_name, shape, ftype in [(\"A\", (M, K), input_float), (\"B\", (K, N), input_float), (\"C\", (M, N), output_float)]:\n",
    "        if arr_name == \"A\":\n",
    "            arrn, arr = sdfg.add_array(name=arr_name, shape=shape, dtype=ftype, storage=global_storage, transient=False, is_hbm_interleaved=is_hbm_interleaved, hbm_split_scheme=hbm_split_scheme[0], hbm_placement_scheme=hbm_placement_scheme[0])\n",
    "            arrs[arrn] = arr\n",
    "        if arr_name == \"B\":\n",
    "            arrn, arr = sdfg.add_array(name=arr_name, shape=shape, dtype=ftype, storage=global_storage, transient=False, is_hbm_interleaved=is_hbm_interleaved, hbm_split_scheme=hbm_split_scheme[1], hbm_placement_scheme=hbm_placement_scheme[1])\n",
    "            arrs[arrn] = arr\n",
    "        if arr_name == \"C\":\n",
    "            arrn, arr = sdfg.add_array(name=arr_name, shape=shape, dtype=ftype, storage=global_storage, transient=False, is_hbm_interleaved=is_hbm_interleaved, hbm_split_scheme=hbm_split_scheme[2], hbm_placement_scheme=hbm_placement_scheme[2])\n",
    "            arrs[arrn] = arr\n",
    "    arrn, arr = sdfg.add_array(name=\"accumulator\", shape=(coarsening_factor*coarsening_factor, tM//coarsening_factor, tN//coarsening_factor), dtype=ftype, storage=local_storage, transient=True)\n",
    "    arrs[arrn] = arr\n",
    "\n",
    "    dev_map_entry, dev_map_exit = main_state.add_map(\n",
    "        name=\"gemm_entry\",\n",
    "        ndrange={\"i\" : dace.subsets.Range([(0, M-1, tM*gM)]),\n",
    "                 \"j\" : dace.subsets.Range([(0, N-1, tN*gN)])},\n",
    "        schedule=device_schedule\n",
    "    )\n",
    "    i = dace.symbol(\"i\")\n",
    "    j = dace.symbol(\"j\")\n",
    "\n",
    "    for name in [\"A\", \"B\", \"C\"]:\n",
    "    # for name in [\"A\", \"B\"]:\n",
    "        if name == \"A\" or name == \"B\":\n",
    "            access_str = \", \".join([f\"0:{n}\" for n in arrs[name].shape])\n",
    "            an = state.add_access(name)\n",
    "            state.add_edge(an, None, dev_map_entry, f\"IN_{name}\", dace.memlet.Memlet(f\"{name}[{access_str}]\"))\n",
    "            dev_map_entry.add_in_connector(f\"IN_{name}\")\n",
    "        if name == \"C\":\n",
    "            access_str = \", \".join([f\"0:{n}\" for n in arrs[name].shape])\n",
    "            # an = state.add_access(name)\n",
    "            dev_map_exit.add_out_connector(f\"OUT_{name}\")\n",
    "            anc3 = state.add_access(name)\n",
    "            state.add_edge(dev_map_exit, f\"OUT_{name}\", anc3, None, dace.memlet.Memlet(f\"{name}[{access_str}]\"))\n",
    "\n",
    "    thread_group_map_entry, thread_group_map_exit = main_state.add_map(\n",
    "        name=\"thread_group_mmad\",\n",
    "        ndrange={\"gi\" : dace.subsets.Range([(0, gM-1, 1)]),\n",
    "                 \"gj\" : dace.subsets.Range([(0, gN-1, 1)])},\n",
    "        schedule=thread_group_schedule\n",
    "    )\n",
    "\n",
    "    gi = dace.symbol(\"gi\")\n",
    "    gj = dace.symbol(\"gj\")\n",
    "    \n",
    "    for name in [\"A\", \"B\", \"C\"]:\n",
    "        if name == \"A\" or name == \"B\":\n",
    "            if name == \"A\":\n",
    "                access_str = \", \".join([f\"i:i + {tM} * {gM}\", \"0:K\"])\n",
    "            elif name == \"B\":\n",
    "                access_str = \", \".join([\"0:K\", f\"j:j + {tN} * {gN}\"])\n",
    "            elif name == \"C\":\n",
    "                access_str = \", \".join([f\"i:i + {gM} * {tM}\", f\"j:j + {gN} * {tN}\"])\n",
    "            state.add_edge(dev_map_entry, f\"OUT_{name}\", thread_group_map_entry, f\"IN_{name}\", dace.memlet.Memlet(f\"{name}[{access_str}]\"))\n",
    "            dev_map_entry.add_out_connector(f\"OUT_{name}\")\n",
    "            thread_group_map_entry.add_in_connector(f\"IN_{name}\")\n",
    "        if name == \"C\":\n",
    "            access_str = \", \".join([f\"i:i + {gM} * {tM}\", f\"j:j + {gN} * {tN}\"])\n",
    "            state.add_edge(thread_group_map_exit, f\"OUT_{name}\", dev_map_exit, f\"IN_{name}\", dace.memlet.Memlet(f\"{name}[{access_str}]\"))\n",
    "            dev_map_exit.add_in_connector(f\"IN_{name}\")\n",
    "            thread_group_map_exit.add_out_connector(f\"OUT_{name}\")\n",
    "\n",
    "    thread_coarsened_map_entry, thread_coarsened_map_exit = main_state.add_map(\n",
    "        name=\"thread_coarsened\",\n",
    "        ndrange={\"ci\" : dace.subsets.Range([(0, tM-1, tM//coarsening_factor)]),\n",
    "                 \"cj\" : dace.subsets.Range([(0, tN-1, tN//coarsening_factor)])},\n",
    "        schedule=dace.dtypes.ScheduleType.SoftHier_Sequential\n",
    "    )\n",
    "\n",
    "    for name in [\"A\", \"B\", \"C\"]:\n",
    "        if name == \"A\" or name == \"B\":\n",
    "                if name == \"A\":\n",
    "                    access_str = \", \".join([f\"i + gi * {tM}:i + gi * {tM} + {tM}\", \"0:K\"])\n",
    "                elif name == \"B\":\n",
    "                    access_str = \", \".join([\"0:K\", f\"j + gj * {tN}:j + gj * {tN} + {tN}\"])\n",
    "                elif name == \"C\":\n",
    "                    access_str = \", \".join([f\"i + gj * {tM}:i + gj * {tM} + {tM}\", f\"j + gj * {tN}:j + gj * {tN} + {tN}\"])\n",
    "                state.add_edge(thread_group_map_entry, f\"OUT_{name}\", thread_coarsened_map_entry, f\"IN_{name}\", dace.memlet.Memlet(f\"{name}[{access_str}]\"))\n",
    "                thread_group_map_entry.add_out_connector(f\"OUT_{name}\")\n",
    "                thread_coarsened_map_entry.add_in_connector(f\"IN_{name}\")\n",
    "        if name == \"C\":\n",
    "            access_str = \", \".join([f\"i + gj * {tM}:i + gj * {tM} + {tM}\", f\"j + gj * {tN}:j + gj * {tN} + {tN}\"])\n",
    "            state.add_edge(thread_coarsened_map_exit, f\"OUT_{name}\", thread_group_map_exit, f\"IN_{name}\", dace.memlet.Memlet(f\"{name}[{access_str}]\"))\n",
    "            thread_group_map_exit.add_in_connector(f\"IN_{name}\")\n",
    "            thread_coarsened_map_exit.add_out_connector(f\"OUT_{name}\")\n",
    "\n",
    "    block_tiled_map_entry, block_tiled_map_exit = main_state.add_map(\n",
    "        name=\"block_tiled\",\n",
    "        ndrange={\"bK\" : dace.subsets.Range([(0, K-1, tK//coarsening_factor)])},\n",
    "        schedule=dace.dtypes.ScheduleType.SoftHier_Sequential\n",
    "    )\n",
    "\n",
    "    accumulator_an = state.add_access(\"accumulator\")\n",
    "    accumulator_an.setzero = True\n",
    "    state.add_edge(thread_coarsened_map_entry, None, accumulator_an, None, dace.memlet.Memlet(None))\n",
    "    access_str = \", \".join([f\"0:{coarsening_factor}*{coarsening_factor}\", f\"0:{tM//coarsening_factor}\", f\"0:{tN//coarsening_factor}\"])\n",
    "    state.add_edge(accumulator_an, None, block_tiled_map_entry, f\"IN_accumulator\", dace.memlet.Memlet(f\"accumulator[{access_str}]\"))\n",
    "    block_tiled_map_entry.add_in_connector(\"IN_accumulator\")\n",
    "    thread_group_map_entry\n",
    "\n",
    "\n",
    "    for name in [\"A\", \"B\"]:\n",
    "        if name == \"A\":\n",
    "            access_str = \", \".join([f\"i + gi * {tM} + ci * {tM//coarsening_factor}:i + gi * {tM} + ci * {tM//coarsening_factor} + {tM//coarsening_factor}\", \"0:K\"])\n",
    "        elif name == \"B\":\n",
    "            access_str = \", \".join([\"0:K\", f\"j + gj * {tN} + cj * {tN//coarsening_factor}:j + gj * {tN} + cj * {tN//coarsening_factor} + {tN//coarsening_factor}\"])\n",
    "\n",
    "        state.add_edge(thread_coarsened_map_entry, f\"OUT_{name}\", block_tiled_map_entry, f\"IN_{name}\", dace.memlet.Memlet(f\"{name}[{access_str}]\"))\n",
    "        block_tiled_map_entry.add_in_connector(f\"IN_{name}\")\n",
    "        thread_coarsened_map_entry.add_out_connector(f\"OUT_{name}\")\n",
    "\n",
    "\n",
    "    # Load\n",
    "    local_access_nodes = dict()\n",
    "    for name, shape in [(\"A\", (tM//coarsening_factor, tK//coarsening_factor)), (\"B\", (tK//coarsening_factor, tN//coarsening_factor))]:\n",
    "        block_tiled_map_entry.add_out_connector(f\"OUT_{name}\")\n",
    "        arrn, arr = sdfg.add_array(name=f\"local_{name}\", shape=shape, dtype=input_float, storage=local_storage, transient=True)\n",
    "        arrs[arrn] = arr\n",
    "        an = state.add_access(f\"local_{name}\")\n",
    "        local_access_nodes[f\"local_{name}\"] = an\n",
    "        if name == \"A\":\n",
    "            access_str = \", \".join([f\"i + gi * {tM} + ci * {tM//coarsening_factor}:i + gi * {tM} + ci * {tM//coarsening_factor} + {tM//coarsening_factor}\", \n",
    "                                    f\"bK:bK+{tK//coarsening_factor}\"])\n",
    "        elif name == \"B\":\n",
    "            access_str = \", \".join([f\"bK:bK+{tK//coarsening_factor}\", \n",
    "                                    f\"j + gj * {tN} + cj * {tN//coarsening_factor}:j + gj * {tN} + cj * {tN//coarsening_factor} + {tN//coarsening_factor}\"])\n",
    "        state.add_edge(block_tiled_map_entry, f\"OUT_{name}\", an, None, dace.memlet.Memlet(f\"{name}[{access_str}]\"))\n",
    "\n",
    "    # Connect local_A + local_B -> matmul -> accumulator\n",
    "    matmul_tasklet = state.add_tasklet(name=\"mmad_redmule\", inputs={\"_in_local_a\", \"_in_local_b\", \"_in_accumulator\"}, outputs={\"_out_accumulator\"},\n",
    "                                       code=mmad_tasklet_str, language=dace.dtypes.Language.CPP)\n",
    "\n",
    "    #for name in [\"local_A\", \"local_B\", \"accumulate\"]:\n",
    "    #    state.add_edge()\n",
    "\n",
    "    for name, an in local_access_nodes.items():\n",
    "        state.add_edge(an, None, matmul_tasklet, \"_in_\" + name.lower(), dace.memlet.Memlet(name))\n",
    "    state.add_edge(block_tiled_map_entry, f\"OUT_accumulator\", matmul_tasklet, \"_in_accumulator\", dace.memlet.Memlet(\"accumulator\"))\n",
    "    # accumulator_an2 = state.add_access(\"accumulator\")\n",
    "    # state.add_edge(matmul_tasklet, f\"_out_accumulator\", accumulator_an2, None, dace.memlet.Memlet(\"accumulator\"))\n",
    "    # state.add_edge(accumulator_an2, None, block_tiled_map_exit, \"IN_accumulator\", dace.memlet.Memlet(\"accumulator\"))\n",
    "    access_str = \", \".join([f\"0:{coarsening_factor}*{coarsening_factor}\", f\"0:{tM//coarsening_factor}\", f\"0:{tN//coarsening_factor}\"])\n",
    "    state.add_edge(matmul_tasklet, \"_out_accumulator\", block_tiled_map_exit, \"IN_accumulator\", dace.memlet.Memlet(f\"accumulator[{access_str}]\"))\n",
    "    block_tiled_map_entry.add_in_connector(\"IN_accumulator\")\n",
    "    block_tiled_map_exit.add_in_connector(\"IN_accumulator\")\n",
    "    block_tiled_map_entry.add_out_connector(\"OUT_accumulator\")\n",
    "    block_tiled_map_exit.add_out_connector(\"OUT_accumulator\")\n",
    "\n",
    "\n",
    "    # assign_tasklet = state.add_tasklet(name=\"assign\", inputs={\"_in_accumulator\"}, outputs={\"_out_C\"}, code=\"_out_C = _in_accumulator\")\n",
    "    # state.add_edge(block_tiled_map_exit, \"OUT_C\", assign_tasklet, \"_in_accumulator\", dace.memlet.Memlet(\"accumulator\")) , \"_in_C\"\n",
    "\n",
    "    # accumulator_an3 = state.add_access(\"accumulator\")\n",
    "    # state.add_edge(block_tiled_map_exit, f\"OUT_accumulator\", accumulator_an3, None, dace.memlet.Memlet(\"accumulator\"))\n",
    "    # state.add_edge(accumulator_an3, None, assign_tasklet, \"_in_accumulator\", dace.memlet.Memlet(\"accumulator\"))\n",
    "    # state.add_edge(block_tiled_map_exit, f\"OUT_accumulator\", assign_tasklet, \"_in_accumulator\", dace.memlet.Memlet())\n",
    "\n",
    "    # c_an2 = state.add_access(\"C\")\n",
    "    accumulator_an3 = state.add_access(\"accumulator\")\n",
    "    \n",
    "    # state.add_edge(assign_tasklet, \"_out_C\", c_an2, None, dace.memlet.Memlet(f\"C[{access_str}]\"))\n",
    "    # thread_coarsened_map_entry.add_out_connector(f\"OUT_C\")\n",
    "    # state.add_edge(c_an2, None, thread_coarsened_map_exit, \"IN_C\", dace.memlet.Memlet(f\"C[{access_str}]\"))\n",
    "    # state.add_edge(assign_tasklet, \"_out_C\", thread_coarsened_map_exit, \"IN_C\", dace.memlet.Memlet(f\"C[{access_str}]\"))\n",
    "    # state.add_edge(block_tiled_map_exit, f\"OUT_accumulator\", thread_coarsened_map_exit, \"IN_C\", dace.memlet.Memlet(f\"C[{access_str}]\"))\n",
    "    access_str = \", \".join([f\"0:{coarsening_factor}*{coarsening_factor}\", f\"0:{tM//coarsening_factor}\", f\"0:{tN//coarsening_factor}\"])\n",
    "    state.add_edge(block_tiled_map_exit, f\"OUT_accumulator\", accumulator_an3, None, dace.memlet.Memlet(f\"accumulator[{access_str}]\"))\n",
    "    access_str = \", \".join([f\"i + gi * {tM} + ci * {tM//coarsening_factor}:i + gi * {tM} + ci * {tM//coarsening_factor} + {tM//coarsening_factor}\", \n",
    "                            f\"j + gj * {tN} + cj * {tN//coarsening_factor}:j + gj * {tN} + cj * {tN//coarsening_factor} + {tN//coarsening_factor}\"])\n",
    "    state.add_edge(accumulator_an3, None, thread_coarsened_map_exit, \"IN_C\", dace.memlet.Memlet(f\"C[{access_str}]\"))\n",
    "    thread_coarsened_map_exit.add_in_connector(\"IN_C\")\n",
    "\n",
    "\n",
    "    (pre_shift_code_block, \n",
    "    BSP_stride,\n",
    "    BSP_init_code_block, \n",
    "    BSP_loop_code_block, \n",
    "    BSP_compute_code_block, \n",
    "    BSP_communication_code_block, \n",
    "    BSP_sync, \n",
    "    post_shift_code_block) = generate_summa_BSP(i, j, gi, gj, gM, gN, tM, tN, tK, M, N, K)\n",
    "    # BSP_compute_code_block = None\n",
    "    BSPTransformer.apply_to(sdfg, accumulator=accumulator_an, map_entry=block_tiled_map_entry, transient=local_access_nodes[\"local_A\"], \n",
    "                            options={\"npe_x\": gM, \"npe_y\": gN, \n",
    "                                     \"gi\": gi, \"gj\": gj,\n",
    "                                    \"i\": i, \"j\": j,\n",
    "                                    \"M\": M, \"N\": N, \"K\": K,\n",
    "                                    \"tM\": tM, \"tN\": tN, \"tK\": tK,\n",
    "                                    \"pre_shift\": pre_shift_code_block,\n",
    "                                    \"BSP_stride\": BSP_stride,\n",
    "                                    \"BSP_init\": BSP_init_code_block,\n",
    "                                    \"BSP_loop\": BSP_loop_code_block,\n",
    "                                    \"BSP_compute\": BSP_compute_code_block,\n",
    "                                    \"BSP_communication\": BSP_communication_code_block,\n",
    "                                    \"BSP_sync\": BSP_sync,\n",
    "                                    \"post_shift\": post_shift_code_block,})\n",
    "    \n",
    "    return sdfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "M = dace.symbol(\"M\")\n",
    "N = dace.symbol(\"N\")\n",
    "K = dace.symbol(\"K\")\n",
    "\n",
    "dim_x = 16\n",
    "dim_y = 4\n",
    "cluster_dims = (dim_x, dim_y)\n",
    "\n",
    "K = 2048\n",
    "M = 4096\n",
    "N = 4096\n",
    "\n",
    "tM = 256\n",
    "tN = 256\n",
    "tK = 64\n",
    "\n",
    "A_host = np.ones((M, K), dtype=np.float16)\n",
    "B_host = np.ones((K, N), dtype=np.float16)\n",
    "C_host = np.zeros((M, N), dtype=np.float16)\n",
    "\n",
    "# for i in range(M):\n",
    "#     for j in range(K):\n",
    "#         if np.random.rand() < 0.3:\n",
    "#             A_host[i, j] = 0.0\n",
    "# for i in range(K):\n",
    "#     for j in range(N):\n",
    "#         if np.random.rand() < 0.3:\n",
    "#             B_host[i, j] = 0.0\n",
    "\n",
    "# G = A_host@B_host\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placement_scheme_A: (24, 25, 26, 27, 28, 29, 30, 31, 24, 25, 26, 27, 28, 29, 30, 31)\n",
      "placement_scheme_B: (0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7)\n",
      "placement_scheme_C: (0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7)\n",
      "array_start_addr_in_channel: 0x40\n",
      "array_start_addr_in_channel: 0x40\n",
      "array_start_addr_in_channel: 0x200040\n",
      "arg: 0x40\n",
      "arg: 0x40\n",
      "arg: 0x200040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([     64,      64, 2097216], dtype=uint32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_handler = InterleaveHandler(array=A_host, block_shape=(tM, tK), cluster_dims=cluster_dims)\n",
    "A_handler.split_horizental()\n",
    "A_handler.place_to_range(place_range=(24, 31, 1))\n",
    "split_scheme_A = A_handler.split_scheme\n",
    "placement_scheme_A = A_handler.placement_scheme\n",
    "\n",
    "B_handler = InterleaveHandler(array=B_host, block_shape=(tK, tN), cluster_dims=cluster_dims)\n",
    "B_handler.split_vertical()\n",
    "B_handler.place_to_range(place_range=(0, 7, 1))\n",
    "split_scheme_B = B_handler.split_scheme\n",
    "placement_scheme_B = B_handler.placement_scheme\n",
    "\n",
    "\n",
    "C_handler = InterleaveHandler(array=C_host, block_shape=(tM, tN), cluster_dims=cluster_dims)\n",
    "C_handler.split_vertical()\n",
    "C_handler.place_to_range(place_range=(0, 7, 1))\n",
    "split_scheme_C = C_handler.split_scheme\n",
    "placement_scheme_C = C_handler.placement_scheme\n",
    "\n",
    "print(f\"placement_scheme_A: {placement_scheme_A}\")\n",
    "print(f\"placement_scheme_B: {placement_scheme_B}\") \n",
    "print(f\"placement_scheme_C: {placement_scheme_C}\")\n",
    "\n",
    "make_preload_elf_hbm_interleaved_new(\"./output.elf\", [A_handler, B_handler, C_handler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/dace4softhier/dace_devel/dace/sdfg/validation.py:538: UserWarning: WARNING: Use of uninitialized transient \"local_A\" in state \"compute\"\n",
      "  warnings.warn('WARNING: Use of uninitialized transient \"%s\" in state \"%s\"' %\n",
      "/scratch/dace4softhier/dace_devel/dace/sdfg/validation.py:538: UserWarning: WARNING: Use of uninitialized transient \"local_B\" in state \"compute\"\n",
      "  warnings.warn('WARNING: Use of uninitialized transient \"%s\" in state \"%s\"' %\n"
     ]
    }
   ],
   "source": [
    "sdfg = _my_gen_matmul_sdfg(hardware_matmul_mnk=(tM, tN, tK),\n",
    "                            global_storage=dace.dtypes.StorageType.SoftHier_HBM,\n",
    "                            local_storage=dace.dtypes.StorageType.SoftHier_TCDM,\n",
    "                            device_schedule=dace.dtypes.ScheduleType.SoftHier_Device,\n",
    "                            thread_group_schedule=dace.dtypes.ScheduleType.SoftHier_Cluster,\n",
    "                            thread_group_dims=cluster_dims,\n",
    "                            hbm_split_scheme=[split_scheme_A, split_scheme_B, split_scheme_C],\n",
    "                            hbm_placement_scheme=[placement_scheme_A, placement_scheme_B, placement_scheme_C],\n",
    "                            is_hbm_interleaved=False,\n",
    "                            input_float=dace.float16,\n",
    "                            output_float=dace.float16,\n",
    "                            coarsening_factor=1,\n",
    "                            mmad_tasklet_str=\"flex_redmule_trigger(_in_local_a, _in_local_b, _in_accumulator, REDMULE_NONE_16);\")\n",
    "sdfg.save(\"my_gemm.sdfgz\")\n",
    "sdfg.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waring: No `gpu_block_size` property specified on map gemm_entry. \n",
      "RedMule Dims [[1, 256, 64], [1, 64, 256]]\n",
      "Generating NestedSDFG using SoftHierCodeGen\n",
      "Generating Tasklet Using CPU Codegen\n",
      "Waring: No `gpu_block_size` property specified on map gemm_entry. \n",
      "RedMule Dims [[1, 256, 64], [1, 64, 256]]\n",
      "Generating NestedSDFG using SoftHierCodeGen\n",
      "Generating Tasklet Using CPU Codegen\n",
      "GVSOC_INSTALL_PATH: None\n",
      "GVSOC_DIR: None\n",
      "SOFTHIER_INSTALL_PATH: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dace.codegen.compiled_sdfg.CompiledSDFG at 0x7f0c3c9f7010>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Code\n",
    "Code(sdfg.generate_code()[1].clean_code, language='cpp')\n",
    "sdfg.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdfg.compile()\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_address = 0x00000000\n",
    "A_address = 64 + start_address\n",
    "B_address = 64 + A_host.nbytes + start_address\n",
    "C_address = 64 + A_host.nbytes + B_host.nbytes + start_address\n",
    "G_address = 64 + A_host.nbytes + B_host.nbytes + C_host.nbytes + start_address\n",
    "# create a uint32 np array to store the addresses\n",
    "args = np.array([A_address, B_address, C_address, K, M, N, G_address], dtype=np.uint32)\n",
    "# args = make_preload_elf_hbm_interleaved(\"output.elf\", [A_host, B_host, C_host], [split_scheme_A, split_scheme_B, split_scheme_C], [placement_scheme_A, placement_scheme_B, placement_scheme_C], [(64, 32), (32, 16), (64, 16)], start_addresses=[])\n",
    "# G = A_host@B_host\n",
    "# make_preload_elf(\"./output.elf\", [args, A_host, B_host, C_host, G])\n",
    "\n",
    "# make_preload_elf(\"/usr/scratch/badile111/dace4softhier/gvsoc/output.elf\", [args, A_host, B_host, C_host])\n",
    "# print(A_host@B_host)\n",
    "# sdfg(A=A_host, B=B_host, C=C_host, M=M, N=N, K=K)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
