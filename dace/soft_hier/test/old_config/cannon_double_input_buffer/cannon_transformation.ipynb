{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://spcl.github.io/dace-webclient/dist/sdfv.js\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dace\n",
    "import typing\n",
    "import os\n",
    "import numpy as np\n",
    "from dace.transformation.dataflow import DoubleBuffering, MapTiling\n",
    "from dace.transformation.soft_hier import CannonTransformer, SplitHBMLoad, CanonTransformer\n",
    "\n",
    "def make_preload_elf(output_file_path, np_arrays, start_addresses=None):\n",
    "    \"\"\"\n",
    "    Generate an ELF file preloading numpy arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - output_file_path (str): Path to save the output ELF file.\n",
    "    - np_arrays (list of numpy.ndarray): List of numpy arrays to include in the ELF.\n",
    "    - start_addresses (list of int or None): List of starting addresses for each array, or None.\n",
    "      If None, addresses are auto-determined with 64-byte alignment.\n",
    "    \"\"\"\n",
    "    NP_DTYPE_TO_C = {\n",
    "        np.dtype('int8'): 'int8_t',\n",
    "        np.dtype('uint8'): 'uint8_t',\n",
    "        np.dtype('int16'): 'int16_t',\n",
    "        np.dtype('uint16'): 'uint16_t',\n",
    "        np.dtype('int32'): 'int32_t',\n",
    "        np.dtype('uint32'): 'uint32_t',\n",
    "        np.dtype('int64'): 'int64_t',\n",
    "        np.dtype('uint64'): 'uint64_t',\n",
    "        np.dtype('float16'): 'float16',\n",
    "        np.dtype('float32'): 'float',\n",
    "        np.dtype('float64'): 'double',\n",
    "    }\n",
    "    \n",
    "    ENV_PATH = os.environ.get(\"PATH\")\n",
    "    # Add RISC-V toolchain to PATH /scratch/dace4softhier/gvsoc/third_party/toolchain/v1.0.16-pulp-riscv-gcc-centos-7/bin/\n",
    "    os.environ[\"PATH\"] = f\"{ENV_PATH}:/usr/scratch/badile111/dace4softhier/gvsoc/third_party/toolchain/v1.0.16-pulp-riscv-gcc-centos-7/bin/\"\n",
    "    \n",
    "    # Handle default for start_addresses\n",
    "    if start_addresses is None:\n",
    "        start_addresses = [None] * len(np_arrays)\n",
    "\n",
    "    # Validate inputs\n",
    "    if len(np_arrays) != len(start_addresses):\n",
    "        raise ValueError(\"np_arrays and start_addresses must have the same length.\")\n",
    "\n",
    "    # 64-byte alignment\n",
    "    alignment = 64\n",
    "    current_address = 0xc0000000  # Default starting address for auto-addressing\n",
    "\n",
    "    # Step 1: Create \"array.c\"\n",
    "    array_c_content = ['#include <stdint.h>']\n",
    "    section_names = []\n",
    "\n",
    "    for idx, (array, start_addr) in enumerate(zip(np_arrays, start_addresses)):\n",
    "        # Determine C type from NumPy dtype\n",
    "        c_type = NP_DTYPE_TO_C.get(array.dtype, None)\n",
    "        if c_type is None:\n",
    "            raise TypeError(f\"Unsupported NumPy dtype: {array.dtype}\")\n",
    "\n",
    "        section_name = f\".custom_section_{idx}\"\n",
    "        section_names.append(section_name)\n",
    "        \n",
    "        if start_addr is None:\n",
    "            # Auto-determine the address with alignment\n",
    "            start_addr = (current_address + alignment - 1) & ~(alignment - 1)\n",
    "        else:\n",
    "            # Ensure provided addresses are aligned\n",
    "            if start_addr % alignment != 0:\n",
    "                raise ValueError(f\"Provided address {start_addr} is not {alignment}-byte aligned.\")\n",
    "\n",
    "        # Generate the array definition\n",
    "        array_values = \", \".join(map(str, array.flatten()))\n",
    "        array_c_content.append(\n",
    "            f'{c_type} array_{idx}[] __attribute__((section(\"{section_name}\"))) = {{{array_values}}};'\n",
    "        )\n",
    "\n",
    "        current_address = start_addr + array.nbytes\n",
    "\n",
    "    array_c_code = \"\\n\".join(array_c_content)\n",
    "\n",
    "    with open(\"array.c\", \"w\") as f:\n",
    "        f.write(array_c_code)\n",
    "\n",
    "\n",
    "    # Step 2: Create \"link.ld\"\n",
    "    link_ld_content = [\"SECTIONS {\"]\n",
    "    current_address = 0xc0000000  # Reset for linker script auto-addressing\n",
    "\n",
    "    for idx, (array, start_addr) in enumerate(zip(np_arrays, start_addresses)):\n",
    "        section_name = section_names[idx]\n",
    "\n",
    "        if start_addr is None:\n",
    "            # Auto-determine the address with alignment\n",
    "            start_addr = (current_address + alignment - 1) & ~(alignment - 1)\n",
    "        link_ld_content.append(\n",
    "            f\"    . = 0x{start_addr:X};\\n    {section_name} : {{ *({section_name}) }}\"\n",
    "        )\n",
    "        current_address = start_addr + array.nbytes\n",
    "\n",
    "    link_ld_content.append(\"}\")\n",
    "    link_ld_code = \"\\n\".join(link_ld_content)\n",
    "\n",
    "    with open(\"link.ld\", \"w\") as f:\n",
    "        f.write(link_ld_code)\n",
    "\n",
    "    # Step 3: Compile the ELF file\n",
    "    os.system(\"riscv32-unknown-elf-gcc -c array.c -o array.o\")\n",
    "    os.system(f\"riscv32-unknown-elf-ld -T link.ld array.o -o {output_file_path}\")\n",
    "    os.system(f\"riscv32-unknown-elf-strip --remove-section=.comment --remove-section=.Pulp_Chip.Info {output_file_path}\")\n",
    "\n",
    "    # Step 4: Cleanup\n",
    "    os.remove(\"array.c\")\n",
    "    os.remove(\"link.ld\")\n",
    "    os.remove(\"array.o\")\n",
    "\n",
    "\n",
    "def make_preload_elf_hbm_interleaved(output_file_path, np_arrays, split_schemes, placement_schemes, hardware_block_sizes, start_addresses=None, KMN=None, args_only=True):\n",
    "    \"\"\"\n",
    "    Split np arrays into tiles and blocks and then use make_preload_elf to generate an ELF file preloading numpy arrays.\n",
    "\n",
    "    \"\"\"\n",
    "     # 1) Combine relevant info with original indices\n",
    "    arrays_info = [\n",
    "        (idx, array, split_schemes[idx], placement_schemes[idx], hardware_block_sizes[idx])\n",
    "        for idx, array in enumerate(np_arrays)\n",
    "    ]\n",
    "\n",
    "    # Helper to compute the number of channels from a placement scheme\n",
    "    def get_num_channels(placement_scheme):\n",
    "        start_channel, end_channel, stride = placement_scheme\n",
    "        return (end_channel - start_channel + 1) // stride\n",
    "\n",
    "    # 2) Sort by number of channels used, descending\n",
    "    arrays_info.sort(key=lambda x: get_num_channels(x[3]), reverse=True)\n",
    "    #        index: x[0]\n",
    "    #        array: x[1]\n",
    "    # split_scheme: x[2]\n",
    "    #placement_scheme: x[3]\n",
    "    #hardware_block_size: x[4]\n",
    "\n",
    "    # 3) Prepare channel start addresses\n",
    "    total_channels = 8\n",
    "    current_start_address = 64\n",
    "    start_addr_in_each_channel = [current_start_address] * total_channels\n",
    "\n",
    "    # We'll store each array's start address in a list keyed by original index\n",
    "    # so we can return them in the original order.\n",
    "    start_addresses_in_original_order = [None] * len(np_arrays)\n",
    "\n",
    "    # 4) Allocate arrays in sorted order\n",
    "    for idx, array, split_scheme, placement_scheme, hardware_block_size in arrays_info:\n",
    "        start_channel, end_channel, stride = placement_scheme\n",
    "        num_channels = get_num_channels(placement_scheme)\n",
    "\n",
    "        # Basic tile check\n",
    "        array_shape = array.shape\n",
    "        tile_height = array_shape[0] // split_scheme[0]\n",
    "        tile_length = array_shape[1] // split_scheme[1]\n",
    "        if tile_height < hardware_block_size[0] or tile_length < hardware_block_size[1]:\n",
    "            raise ValueError(\n",
    "                f\"Invalid tile size: {tile_height}x{tile_length}\"\n",
    "                f\" < {hardware_block_size[0]}x{hardware_block_size[1]}\"\n",
    "            )\n",
    "\n",
    "        # Compute how many bytes total and how many bytes per channel\n",
    "        array_size = array.nbytes\n",
    "        array_size_per_channel = array_size // num_channels\n",
    "\n",
    "        # Grab the start address for the \"first\" channel in the group\n",
    "        array_start_addr_in_channel = start_addr_in_each_channel[start_channel]\n",
    "\n",
    "        # 5) Store that start address in the array's original position\n",
    "        start_addresses_in_original_order[idx] = array_start_addr_in_channel\n",
    "\n",
    "        # 6) Advance addresses in all channels used by this array\n",
    "        for i in range(num_channels):\n",
    "            channel_idx = start_channel + i * stride\n",
    "\n",
    "            # Optional: Validate all channels in the group are at the same start\n",
    "            if start_addr_in_each_channel[channel_idx] != array_start_addr_in_channel:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid placement scheme: {placement_scheme}, \"\n",
    "                    f\"channel {channel_idx} start address mismatch\"\n",
    "                )\n",
    "            start_addr_in_each_channel[channel_idx] += array_size_per_channel\n",
    "\n",
    "    args = start_addresses_in_original_order\n",
    "    for arg in args:\n",
    "        print(f\"arg: {hex(arg)}\")\n",
    "\n",
    "      \n",
    "    split_arrays = []\n",
    "    split_arrays.append(args)\n",
    "    split_arrays_start_addresses = []\n",
    "    split_arrays_start_addresses.append(hbm_node_addr_base) # for store args\n",
    "\n",
    "    if not args_only:\n",
    "        for array, split_scheme, placement_scheme, hardware_block_size, arg_start_addr in zip(np_arrays, split_schemes, placement_schemes, hardware_block_sizes, args):\n",
    "            current_start_address = arg_start_addr\n",
    "            print(f\"current_start_address: {hex(current_start_address)}\")\n",
    "            block_height = hardware_block_size[0]\n",
    "            block_length = hardware_block_size[1]\n",
    "            block_size = block_height * block_length * np.dtype(array.dtype).itemsize\n",
    "            print(f\"block_size: {block_size}\")\n",
    "            channel_start = placement_scheme[0]\n",
    "            print(f\"channel_start: {channel_start}\")\n",
    "            channel_end = placement_scheme[1]\n",
    "            print(f\"channel_end: {channel_end}\")\n",
    "            channel_stride = placement_scheme[2]\n",
    "            print(f\"channel_stride: {channel_stride}\")\n",
    "            num_channels = (channel_end - channel_start + 1) // channel_stride\n",
    "            array_shape = array.shape\n",
    "            print(f\"array_shape: {array_shape}\")\n",
    "            tile_height = array_shape[0] // split_scheme[0]\n",
    "            print(f\"tile_height: {tile_height}\")\n",
    "            tile_length = array_shape[1] // split_scheme[1]\n",
    "            print(f\"tile_length: {tile_length}\")\n",
    "            tile_size = tile_length * tile_height * np.dtype(array.dtype).itemsize\n",
    "            print(f\"tile_size: {tile_size}\")\n",
    "            for i in range(split_scheme[0]):\n",
    "                for j in range(split_scheme[1]):\n",
    "                    tile_idx = i * split_scheme[1] + j\n",
    "                    print(f\"tile_idx: {tile_idx}\")\n",
    "                    channel_offset = tile_idx % num_channels\n",
    "                    print(f\"channel_offset: {channel_offset}\")\n",
    "                    channel_idx = channel_start + channel_offset * channel_stride\n",
    "                    print(f\"channel_idx: {channel_idx}\")\n",
    "                    tile = array[i*tile_height:(i+1)*tile_height, j*tile_length:(j+1)*tile_length]\n",
    "                    for bi in range(0, tile_height, block_height):\n",
    "                        for bj in range(0, tile_length, block_length):\n",
    "                            print(f\"bi: {bi}, bj: {bj}\")\n",
    "                            block = tile[bi:bi+block_height, bj:bj+block_length] \n",
    "                            split_arrays.append(block)\n",
    "                            bi_index = bi // block_height\n",
    "                            bj_index = bj // block_length\n",
    "                            block_address = hbm_node_addr_base + current_start_address + channel_idx * hbm_node_addr_space + (tile_idx // num_channels) * tile_size + (bi_index * tile_length // block_length + bj_index) * block_size\n",
    "                            print(f\"block_address: {hex(block_address)}\")\n",
    "                            split_arrays_start_addresses.append(block_address)\n",
    "\n",
    "    if KMN is None:\n",
    "        args.append(K)\n",
    "        args.append(M)\n",
    "        args.append(N)\n",
    "    else:\n",
    "        args.append(KMN[0])\n",
    "        args.append(KMN[1])\n",
    "        args.append(KMN[2])\n",
    "    # args to np arrays\n",
    "    args = np.array(args, dtype=np.uint32)  \n",
    "\n",
    "    # replace the args in split_arrays with new args\n",
    "    split_arrays[0] = args\n",
    "\n",
    "    make_preload_elf(output_file_path, split_arrays, split_arrays_start_addresses)\n",
    "\n",
    "    return args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s_local_B', 's_local_A'}\n",
      "{'s_local_B': 'local_B', 's_local_A': 'local_A'}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "M = dace.symbol(\"M\")\n",
    "N = dace.symbol(\"N\")\n",
    "K = dace.symbol(\"K\")\n",
    "\n",
    "def _my_gen_matmul_sdfg(hardware_matmul_mnk: typing.Tuple,\n",
    "                     global_storage: dace.dtypes.StorageType,\n",
    "                     local_storage: dace.dtypes.StorageType,\n",
    "                     device_schedule: dace.dtypes.ScheduleType,\n",
    "                     thread_group_schedule: dace.dtypes.ScheduleType,\n",
    "                     thread_group_dims: typing.Tuple,\n",
    "                     hbm_split_scheme: typing.List[typing.Tuple[int, int]],\n",
    "                     hbm_placement_scheme: typing.List[typing.Tuple[int, int]],\n",
    "                     input_float,\n",
    "                     output_float,\n",
    "                     coarsening_factor,\n",
    "                     mmad_tasklet_str: str,\n",
    "                     is_hbm_interleaved: bool = False):\n",
    "    sdfg = dace.SDFG(\"GEMM\")\n",
    "    tM, tN, tK = hardware_matmul_mnk\n",
    "    tM *= coarsening_factor\n",
    "    tN *= coarsening_factor\n",
    "    tK *= coarsening_factor\n",
    "    gM, gN = thread_group_dims\n",
    "\n",
    "    main_state = sdfg.add_state(\"main\")\n",
    "    state = main_state\n",
    "\n",
    "    arrs = dict()\n",
    "    for arr_name, shape, ftype in [(\"A\", (M, K), input_float), (\"B\", (K, N), input_float), (\"C\", (M, N), output_float)]:\n",
    "        if arr_name == \"A\":\n",
    "            arrn, arr = sdfg.add_array(name=arr_name, shape=shape, dtype=ftype, storage=global_storage, transient=False, is_hbm_interleaved=is_hbm_interleaved, hbm_split_scheme=hbm_split_scheme[0], hbm_placement_scheme=hbm_placement_scheme[0])\n",
    "            arrs[arrn] = arr\n",
    "        if arr_name == \"B\":\n",
    "            arrn, arr = sdfg.add_array(name=arr_name, shape=shape, dtype=ftype, storage=global_storage, transient=False, is_hbm_interleaved=is_hbm_interleaved, hbm_split_scheme=hbm_split_scheme[1], hbm_placement_scheme=hbm_placement_scheme[1])\n",
    "            arrs[arrn] = arr\n",
    "        if arr_name == \"C\":\n",
    "            arrn, arr = sdfg.add_array(name=arr_name, shape=shape, dtype=ftype, storage=global_storage, transient=False, is_hbm_interleaved=is_hbm_interleaved, hbm_split_scheme=hbm_split_scheme[2], hbm_placement_scheme=hbm_placement_scheme[2])\n",
    "            arrs[arrn] = arr\n",
    "    arrn, arr = sdfg.add_array(name=\"accumulator\", shape=(coarsening_factor*coarsening_factor, tM//coarsening_factor, tN//coarsening_factor), dtype=ftype, storage=local_storage, transient=True)\n",
    "    arrs[arrn] = arr\n",
    "\n",
    "    dev_map_entry, dev_map_exit = main_state.add_map(\n",
    "        name=\"gemm_entry\",\n",
    "        ndrange={\"i\" : dace.subsets.Range([(0, M-1, tM*gM)]),\n",
    "                 \"j\" : dace.subsets.Range([(0, N-1, tN*gN)])},\n",
    "        schedule=device_schedule\n",
    "    )\n",
    "    for name in [\"A\", \"B\", \"C\"]:\n",
    "    # for name in [\"A\", \"B\"]:\n",
    "        if name == \"A\" or name == \"B\":\n",
    "            access_str = \", \".join([f\"0:{n}\" for n in arrs[name].shape])\n",
    "            an = state.add_access(name)\n",
    "            state.add_edge(an, None, dev_map_entry, f\"IN_{name}\", dace.memlet.Memlet(f\"{name}[{access_str}]\"))\n",
    "            dev_map_entry.add_in_connector(f\"IN_{name}\")\n",
    "        if name == \"C\":\n",
    "            access_str = \", \".join([f\"0:{n}\" for n in arrs[name].shape])\n",
    "            # an = state.add_access(name)\n",
    "            dev_map_exit.add_out_connector(f\"OUT_{name}\")\n",
    "            anc3 = state.add_access(name)\n",
    "            state.add_edge(dev_map_exit, f\"OUT_{name}\", anc3, None, dace.memlet.Memlet(f\"{name}[{access_str}]\"))\n",
    "\n",
    "    thread_group_map_entry, thread_group_map_exit = main_state.add_map(\n",
    "        name=\"thread_group_mmad\",\n",
    "        ndrange={\"gi\" : dace.subsets.Range([(0, gM-1, 1)]),\n",
    "                 \"gj\" : dace.subsets.Range([(0, gM-1, 1)])},\n",
    "        schedule=thread_group_schedule\n",
    "    )\n",
    "\n",
    "    gi = dace.symbol(\"gi\")\n",
    "    gj = dace.symbol(\"gj\")\n",
    "    \n",
    "    for name in [\"A\", \"B\", \"C\"]:\n",
    "        if name == \"A\" or name == \"B\":\n",
    "            if name == \"A\":\n",
    "                access_str = \", \".join([f\"i:i + {tM} * {gM}\", \"0:K\"])\n",
    "            elif name == \"B\":\n",
    "                access_str = \", \".join([\"0:K\", f\"j:j + {tN} * {gN}\"])\n",
    "            elif name == \"C\":\n",
    "                access_str = \", \".join([f\"i:i + {gM} * {tM}\", f\"j:j + {gN} * {tN}\"])\n",
    "            state.add_edge(dev_map_entry, f\"OUT_{name}\", thread_group_map_entry, f\"IN_{name}\", dace.memlet.Memlet(f\"{name}[{access_str}]\"))\n",
    "            dev_map_entry.add_out_connector(f\"OUT_{name}\")\n",
    "            thread_group_map_entry.add_in_connector(f\"IN_{name}\")\n",
    "        if name == \"C\":\n",
    "            access_str = \", \".join([f\"i:i + {gM} * {tM}\", f\"j:j + {gN} * {tN}\"])\n",
    "            state.add_edge(thread_group_map_exit, f\"OUT_{name}\", dev_map_exit, f\"IN_{name}\", dace.memlet.Memlet(f\"{name}[{access_str}]\"))\n",
    "            dev_map_exit.add_in_connector(f\"IN_{name}\")\n",
    "            thread_group_map_exit.add_out_connector(f\"OUT_{name}\")\n",
    "\n",
    "    thread_coarsened_map_entry, thread_coarsened_map_exit = main_state.add_map(\n",
    "        name=\"thread_coarsened\",\n",
    "        ndrange={\"ci\" : dace.subsets.Range([(0, tM-1, tM//coarsening_factor)]),\n",
    "                 \"cj\" : dace.subsets.Range([(0, tN-1, tN//coarsening_factor)])},\n",
    "        schedule=dace.dtypes.ScheduleType.SoftHier_Sequential\n",
    "    )\n",
    "\n",
    "    for name in [\"A\", \"B\", \"C\"]:\n",
    "        if name == \"A\" or name == \"B\":\n",
    "                if name == \"A\":\n",
    "                    access_str = \", \".join([f\"i + gi * {tM}:i + gi * {tM} + {tM}\", \"0:K\"])\n",
    "                elif name == \"B\":\n",
    "                    access_str = \", \".join([\"0:K\", f\"j + gj * {tN}:j + gj * {tN} + {tN}\"])\n",
    "                elif name == \"C\":\n",
    "                    access_str = \", \".join([f\"i + gj * {tM}:i + gj * {tM} + {tM}\", f\"j + gj * {tN}:j + gj * {tN} + {tN}\"])\n",
    "                state.add_edge(thread_group_map_entry, f\"OUT_{name}\", thread_coarsened_map_entry, f\"IN_{name}\", dace.memlet.Memlet(f\"{name}[{access_str}]\"))\n",
    "                thread_group_map_entry.add_out_connector(f\"OUT_{name}\")\n",
    "                thread_coarsened_map_entry.add_in_connector(f\"IN_{name}\")\n",
    "        if name == \"C\":\n",
    "            access_str = \", \".join([f\"i + gj * {tM}:i + gj * {tM} + {tM}\", f\"j + gj * {tN}:j + gj * {tN} + {tN}\"])\n",
    "            state.add_edge(thread_coarsened_map_exit, f\"OUT_{name}\", thread_group_map_exit, f\"IN_{name}\", dace.memlet.Memlet(f\"{name}[{access_str}]\"))\n",
    "            thread_group_map_exit.add_in_connector(f\"IN_{name}\")\n",
    "            thread_coarsened_map_exit.add_out_connector(f\"OUT_{name}\")\n",
    "\n",
    "    block_tiled_map_entry, block_tiled_map_exit = main_state.add_map(\n",
    "        name=\"block_tiled\",\n",
    "        ndrange={\"bK\" : dace.subsets.Range([(0, K-1, tK//coarsening_factor)])},\n",
    "        schedule=dace.dtypes.ScheduleType.SoftHier_Sequential\n",
    "    )\n",
    "\n",
    "    accumulator_an = state.add_access(\"accumulator\")\n",
    "    accumulator_an.setzero = True\n",
    "    state.add_edge(thread_coarsened_map_entry, None, accumulator_an, None, dace.memlet.Memlet(None))\n",
    "    access_str = \", \".join([f\"0:{coarsening_factor}*{coarsening_factor}\", f\"0:{tM//coarsening_factor}\", f\"0:{tN//coarsening_factor}\"])\n",
    "    state.add_edge(accumulator_an, None, block_tiled_map_entry, f\"IN_accumulator\", dace.memlet.Memlet(f\"accumulator[{access_str}]\"))\n",
    "    block_tiled_map_entry.add_in_connector(\"IN_accumulator\")\n",
    "    thread_group_map_entry\n",
    "\n",
    "\n",
    "    for name in [\"A\", \"B\"]:\n",
    "        if name == \"A\":\n",
    "            access_str = \", \".join([f\"i + gi * {tM} + ci * {tM//coarsening_factor}:i + gi * {tM} + ci * {tM//coarsening_factor} + {tM//coarsening_factor}\", \"0:K\"])\n",
    "        elif name == \"B\":\n",
    "            access_str = \", \".join([\"0:K\", f\"j + gj * {tN} + cj * {tN//coarsening_factor}:j + gj * {tN} + cj * {tN//coarsening_factor} + {tN//coarsening_factor}\"])\n",
    "\n",
    "        state.add_edge(thread_coarsened_map_entry, f\"OUT_{name}\", block_tiled_map_entry, f\"IN_{name}\", dace.memlet.Memlet(f\"{name}[{access_str}]\"))\n",
    "        block_tiled_map_entry.add_in_connector(f\"IN_{name}\")\n",
    "        thread_coarsened_map_entry.add_out_connector(f\"OUT_{name}\")\n",
    "\n",
    "\n",
    "    # Load\n",
    "    local_access_nodes = dict()\n",
    "    for name, shape in [(\"A\", (tM//coarsening_factor, tK//coarsening_factor)), (\"B\", (tK//coarsening_factor, tN//coarsening_factor))]:\n",
    "        block_tiled_map_entry.add_out_connector(f\"OUT_{name}\")\n",
    "        arrn, arr = sdfg.add_array(name=f\"local_{name}\", shape=shape, dtype=input_float, storage=local_storage, transient=True)\n",
    "        arrs[arrn] = arr\n",
    "        an = state.add_access(f\"local_{name}\")\n",
    "        local_access_nodes[f\"local_{name}\"] = an\n",
    "        if name == \"A\":\n",
    "            access_str = \", \".join([f\"i + gi * {tM} + ci * {tM//coarsening_factor}:i + gi * {tM} + ci * {tM//coarsening_factor} + {tM//coarsening_factor}\", \n",
    "                                    f\"bK:bK+{tK//coarsening_factor}\"])\n",
    "        elif name == \"B\":\n",
    "            access_str = \", \".join([f\"bK:bK+{tK//coarsening_factor}\", \n",
    "                                    f\"j + gj * {tN} + cj * {tN//coarsening_factor}:j + gj * {tN} + cj * {tN//coarsening_factor} + {tN//coarsening_factor}\"])\n",
    "        state.add_edge(block_tiled_map_entry, f\"OUT_{name}\", an, None, dace.memlet.Memlet(f\"{name}[{access_str}]\"))\n",
    "\n",
    "    # Connect local_A + local_B -> matmul -> accumulator\n",
    "    matmul_tasklet = state.add_tasklet(name=\"mmad_redmule\", inputs={\"_in_local_a\", \"_in_local_b\", \"_in_accumulator\"}, outputs={\"_out_accumulator\"},\n",
    "                                       code=mmad_tasklet_str, language=dace.dtypes.Language.CPP)\n",
    "\n",
    "    #for name in [\"local_A\", \"local_B\", \"accumulate\"]:\n",
    "    #    state.add_edge()\n",
    "\n",
    "    for name, an in local_access_nodes.items():\n",
    "        state.add_edge(an, None, matmul_tasklet, \"_in_\" + name.lower(), dace.memlet.Memlet(name))\n",
    "    state.add_edge(block_tiled_map_entry, f\"OUT_accumulator\", matmul_tasklet, \"_in_accumulator\", dace.memlet.Memlet(\"accumulator\"))\n",
    "    # accumulator_an2 = state.add_access(\"accumulator\")\n",
    "    # state.add_edge(matmul_tasklet, f\"_out_accumulator\", accumulator_an2, None, dace.memlet.Memlet(\"accumulator\"))\n",
    "    # state.add_edge(accumulator_an2, None, block_tiled_map_exit, \"IN_accumulator\", dace.memlet.Memlet(\"accumulator\"))\n",
    "    access_str = \", \".join([f\"0:{coarsening_factor}*{coarsening_factor}\", f\"0:{tM//coarsening_factor}\", f\"0:{tN//coarsening_factor}\"])\n",
    "    state.add_edge(matmul_tasklet, \"_out_accumulator\", block_tiled_map_exit, \"IN_accumulator\", dace.memlet.Memlet(f\"accumulator[{access_str}]\"))\n",
    "    block_tiled_map_entry.add_in_connector(\"IN_accumulator\")\n",
    "    block_tiled_map_exit.add_in_connector(\"IN_accumulator\")\n",
    "    block_tiled_map_entry.add_out_connector(\"OUT_accumulator\")\n",
    "    block_tiled_map_exit.add_out_connector(\"OUT_accumulator\")\n",
    "\n",
    "\n",
    "    # assign_tasklet = state.add_tasklet(name=\"assign\", inputs={\"_in_accumulator\"}, outputs={\"_out_C\"}, code=\"_out_C = _in_accumulator\")\n",
    "    # state.add_edge(block_tiled_map_exit, \"OUT_C\", assign_tasklet, \"_in_accumulator\", dace.memlet.Memlet(\"accumulator\")) , \"_in_C\"\n",
    "\n",
    "    # accumulator_an3 = state.add_access(\"accumulator\")\n",
    "    # state.add_edge(block_tiled_map_exit, f\"OUT_accumulator\", accumulator_an3, None, dace.memlet.Memlet(\"accumulator\"))\n",
    "    # state.add_edge(accumulator_an3, None, assign_tasklet, \"_in_accumulator\", dace.memlet.Memlet(\"accumulator\"))\n",
    "    # state.add_edge(block_tiled_map_exit, f\"OUT_accumulator\", assign_tasklet, \"_in_accumulator\", dace.memlet.Memlet())\n",
    "\n",
    "    # c_an2 = state.add_access(\"C\")\n",
    "    accumulator_an3 = state.add_access(\"accumulator\")\n",
    "    \n",
    "    # state.add_edge(assign_tasklet, \"_out_C\", c_an2, None, dace.memlet.Memlet(f\"C[{access_str}]\"))\n",
    "    # thread_coarsened_map_entry.add_out_connector(f\"OUT_C\")\n",
    "    # state.add_edge(c_an2, None, thread_coarsened_map_exit, \"IN_C\", dace.memlet.Memlet(f\"C[{access_str}]\"))\n",
    "    # state.add_edge(assign_tasklet, \"_out_C\", thread_coarsened_map_exit, \"IN_C\", dace.memlet.Memlet(f\"C[{access_str}]\"))\n",
    "    # state.add_edge(block_tiled_map_exit, f\"OUT_accumulator\", thread_coarsened_map_exit, \"IN_C\", dace.memlet.Memlet(f\"C[{access_str}]\"))\n",
    "    access_str = \", \".join([f\"0:{coarsening_factor}*{coarsening_factor}\", f\"0:{tM//coarsening_factor}\", f\"0:{tN//coarsening_factor}\"])\n",
    "    state.add_edge(block_tiled_map_exit, f\"OUT_accumulator\", accumulator_an3, None, dace.memlet.Memlet(f\"accumulator[{access_str}]\"))\n",
    "    access_str = \", \".join([f\"i + gi * {tM} + ci * {tM//coarsening_factor}:i + gi * {tM} + ci * {tM//coarsening_factor} + {tM//coarsening_factor}\", \n",
    "                            f\"j + gj * {tN} + cj * {tN//coarsening_factor}:j + gj * {tN} + cj * {tN//coarsening_factor} + {tN//coarsening_factor}\"])\n",
    "    state.add_edge(accumulator_an3, None, thread_coarsened_map_exit, \"IN_C\", dace.memlet.Memlet(f\"C[{access_str}]\"))\n",
    "    thread_coarsened_map_exit.add_in_connector(\"IN_C\")\n",
    "    \n",
    "    cannon_nsdfg_node = CannonTransformer.apply_to(sdfg, map_entry=block_tiled_map_entry, transient=local_access_nodes[\"local_A\"], options={\"npe\": gM, \"gi\": gi, \"gj\": gj})\n",
    "    SplitHBMLoad.apply_to(sdfg, map_entry=block_tiled_map_entry, schedule_nsdfg_node = cannon_nsdfg_node, options={\"npe\": gM, \"gi\": gi, \"gj\": gj})\n",
    "\n",
    "    return sdfg\n",
    "\n",
    "\n",
    "\n",
    "split_scheme_A = (2, 2)\n",
    "split_scheme_B = (2, 2)\n",
    "split_scheme_C = (2, 2)\n",
    "placement_scheme_A = (0, 3, 1)\n",
    "placement_scheme_B = (0, 3, 1)\n",
    "placement_scheme_C = (0, 3, 1)\n",
    "tM = 128\n",
    "tN = 128\n",
    "tK = 128\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sdfg = _my_gen_matmul_sdfg(hardware_matmul_mnk=(tM, tN, tK),\n",
    "                            global_storage=dace.dtypes.StorageType.SoftHier_HBM,\n",
    "                            local_storage=dace.dtypes.StorageType.SoftHier_TCDM,\n",
    "                            device_schedule=dace.dtypes.ScheduleType.SoftHier_Device,\n",
    "                            thread_group_schedule=dace.dtypes.ScheduleType.SoftHier_Cluster,\n",
    "                            thread_group_dims=(4, 4),\n",
    "                            hbm_split_scheme=[split_scheme_A, split_scheme_B, split_scheme_C],\n",
    "                            hbm_placement_scheme=[placement_scheme_A, placement_scheme_B, placement_scheme_C],\n",
    "                            is_hbm_interleaved=False,\n",
    "                            input_float=dace.float16,\n",
    "                            output_float=dace.float16,\n",
    "                            coarsening_factor=1,\n",
    "                            mmad_tasklet_str=\"flex_redmule_trigger(_in_local_a, _in_local_b, _in_accumulator, REDMULE_FP_16);\")\n",
    "    sdfg.save(\"my_gemm.sdfgz\")\n",
    "    sdfg.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Code\n",
    "Code(sdfg.generate_code()[1].clean_code, language='cpp')\n",
    "# sdfg.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdfg.compile()\n",
    "# import numpy as np\n",
    "K = 4096\n",
    "M = 512\n",
    "N = 512\n",
    "# A_host = np.random.rand(M, K).astype(np.float16)\n",
    "# B_host = np.random.rand(K, N).astype(np.float16)\n",
    "A_host = np.ones((M, K), dtype=np.float16)\n",
    "B_host = np.ones((K, N), dtype=np.float16)\n",
    "C_host = np.zeros((M, N), dtype=np.float16)\n",
    "\n",
    "# for i in range(M):\n",
    "#     for j in range(K):\n",
    "#         if np.random.rand() < 0.9:\n",
    "#             A_host[i, j] /= 4.0\n",
    "# for i in range(K):\n",
    "#     for j in range(N):\n",
    "#         if np.random.rand() < 0.9:\n",
    "#             B_host[i, j] /= 4.0\n",
    "\n",
    "for i in range(M):\n",
    "    for j in range(K):\n",
    "        if np.random.rand() < 0.7:\n",
    "            A_host[i, j] = 0.0\n",
    "for i in range(K):\n",
    "    for j in range(N):\n",
    "        if np.random.rand() < 0.7:\n",
    "            B_host[i, j] = 0.0\n",
    "\n",
    "G = A_host@B_host\n",
    "\n",
    "start_address = 0x00000000\n",
    "A_address = 64 + start_address\n",
    "B_address = 64 + A_host.nbytes + start_address\n",
    "C_address = 64 + A_host.nbytes + B_host.nbytes + start_address\n",
    "G_address = 64 + A_host.nbytes + B_host.nbytes + C_host.nbytes + start_address\n",
    "# create a uint32 np array to store the addresses\n",
    "args = np.array([A_address, B_address, C_address, K, M, N, G_address], dtype=np.uint32)\n",
    "# args = make_preload_elf_hbm_interleaved(\"output.elf\", [A_host, B_host, C_host], [split_scheme_A, split_scheme_B, split_scheme_C], [placement_scheme_A, placement_scheme_B, placement_scheme_C], [(tM, tK), (tK, tN), (tM, tN)], start_addresses=[])\n",
    "make_preload_elf(\"./output.elf\", [args, A_host, B_host, C_host, G])\n",
    "\n",
    "# make_preload_elf(\"/usr/scratch/badile111/dace4softhier/gvsoc/output.elf\", [args, A_host, B_host, C_host])\n",
    "# print(A_host@B_host)\n",
    "# sdfg(A=A_host, B=B_host, C=C_host, M=M, N=N, K=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(A_host)\n",
    "\n",
    "print(result[0][0])\n",
    "\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
